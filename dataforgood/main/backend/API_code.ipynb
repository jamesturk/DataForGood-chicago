{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "# Base URL for the ACS data\n",
    "base_url = \"https://api.census.gov/data/{year}/acs/acs5\"\n",
    "subject_url = \"https://api.census.gov/data/{year}/acs/acs5/subject\"\n",
    "profile_url = \"https://api.census.gov/data/{year}/acs/acs5/profile\"\n",
    "\n",
    "# Places to change\n",
    "# (1) variables (2) relative path\n",
    "\n",
    "# Variables to fetch\n",
    "\n",
    "# Economics #\n",
    "\n",
    "# Median Income\n",
    "#variables = {'main_median_income': \"NAME,S1903_C03_001E\", 'sub_median_white': \"NAME,S1903_C03_002E\",\n",
    "             #'sub_median_black': \"NAME,S1903_C03_003E\", 'sub_median_ind_ala': \"NAME,S1903_C03_004E\",\n",
    "             #'sub_median_asia': \"NAME,S1903_C03_005E\", 'sub_median_hawai': \"NAME,S1903_C03_006E\",\n",
    "             #'sub_median_other': \"NAME,S1903_C03_007E\"  }\n",
    "\n",
    "# Mean Income\n",
    "#variables = {'main_mean_income': \"NAME,S1902_C03_019E\", 'sub_mean_white': \"NAME,S1902_C03_020E\",\n",
    "             #'sub_mean_black': \"NAME,S1902_C03_021E\", 'sub_mean_ind_ala': \"NAME,S1902_C03_022E\",\n",
    "             #'sub_mean_asia': \"NAME,S1902_C03_023E\", 'sub_mean_hawai': \"NAME,S1902_C03_024E\",\n",
    "             #'sub_mean_other': \"NAME,S1902_C03_025E\"}\n",
    "\n",
    "# Housing #\n",
    "\n",
    "# HouseRent\n",
    "# variables = {'main_agg_rent': \"NAME,B25060_001E\", 'sub_median_rent': \"NAME,B25058_001E\", \n",
    "             #'sub_lower_rent': \"NAME,B25057_001E\", 'sub_upper_rent': \"NAME,B25059_001E\"}\n",
    "# HouseholdType\n",
    "#variables = {'main_household_total': \"NAME,B11001_001E\", 'sub_household_family': \"NAME,B11001_002E\", \n",
    "             #'sub_household_nonfamily': \"NAME,B11001_007E\"}\n",
    "\n",
    "\n",
    "# Education\n",
    "# median earning\n",
    "#variables = {'main_median_earning': \"NAME,S1501_C01_059E\", 'sub_less_high': \"NAME,S1501_C01_060E\",\n",
    "             #'sub_high': \"NAME,S1501_C01_061E\", 'sub_college': \"NAME,S1501_C01_062E\",\n",
    "             #'sub_bachelor': \"NAME,S1501_C01_063E\", 'sub_grad': \"NAME,S1501_C01_064E\"}\n",
    "\n",
    "# enrollment\n",
    "#variables = {'main_enroll': \"NAME,S1401_C01_001E\", 'sub_nursery': \"NAME,S1401_C01_002E\",\n",
    "             #'sub_kind_12': \"NAME,S1401_C01_003E\", 'sub_college': \"NAME,S1401_C01_008E\",\n",
    "             #'sub_grad': \"NAME,S1401_C01_009E\"}\n",
    "\n",
    "# Health #\n",
    "# Disability\n",
    "#variables = {'main_disability': \"NAME,S1810_C02_001E\", 'sub_hearing': \"NAME,S1810_C02_019E\",\n",
    "             #'sub_vision': \"NAME,S1810_C02_029E\", 'sub_cognitive': \"NAME,S1810_C02_039E\",\n",
    "             #'sub_ambulatory': \"NAME,S1810_C02_047E\", 'sub_self_care': \"NAME,S1810_C02_055E\",\n",
    "             #'sub_ind_living': \"NAME,S1810_C02_063E\"}\n",
    "\n",
    "# Insurance\n",
    "#variables = {'main_population': \"NAME,S2701_C01_001E\", 'sub_insured': \"NAME,S2701_C02_001E\",\n",
    "             #'sub_uninsured': \"NAME,S2701_C04_001E\"}\n",
    "\n",
    "# Population #\n",
    "# Races\n",
    "\n",
    "#variables = {'main_population': \"NAME,DP05_0033E\", 'sub_pop_white': \"NAME,DP05_0037E\",\n",
    "             #'sub_pop_black': \"NAME,DP05_0038E\", 'sub_pop_ind_ala': \"NAME,DP05_0039E\",\n",
    "             #'sub_pop_asia': \"NAME,DP05_0044E\", 'sub_pop_hawai': \"NAME,DP05_0052E\",\n",
    "             #'sub_pop_other': \"NAME,DP05_0057E\", 'sub_pop_two': \"NAME,DP05_0058E\"}\n",
    "\n",
    "variables = {'main_median_age': \"NAME,B01002_001E\", 'sub_male_age': \"NAME,B01002_002E\",\n",
    "             'sub_female_age': \"NAME,B01002_003E\"}\n",
    "\n",
    "# Location filters\n",
    "location = \"for=tract:*&in=state:17+county:031\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function-1\n",
    "\n",
    "def extract_info_from_filename(filename, ind_type):\n",
    "    # Assuming filename is something like 'main_2017.csv'\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) > 1:\n",
    "        year_part = parts[-1]  # This would be '2017.csv'\n",
    "        year = year_part.split('.')[0]  # This splits '2017.csv' into '2017' and 'csv' and takes the first part\n",
    "        if ind_type == 'main':\n",
    "            indicator = filename[5:-9]\n",
    "        else:\n",
    "            indicator = filename[4:-9]\n",
    "        if year.isdigit():  # Check if 'year' is all digits\n",
    "            return (indicator, int(year))\n",
    "    return None, None\n",
    "\n",
    "all_dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2017 for main_median_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/main/main_median_age_2017.csv\n",
      "Data for 2018 for main_median_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/main/main_median_age_2018.csv\n",
      "Data for 2019 for main_median_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/main/main_median_age_2019.csv\n",
      "Data for 2020 for main_median_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/main/main_median_age_2020.csv\n",
      "Data for 2021 for main_median_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/main/main_median_age_2021.csv\n",
      "Data for 2022 for main_median_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/main/main_median_age_2022.csv\n",
      "Data for 2017 for sub_male_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_male_age_2017.csv\n",
      "Data for 2018 for sub_male_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_male_age_2018.csv\n",
      "Data for 2019 for sub_male_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_male_age_2019.csv\n",
      "Data for 2020 for sub_male_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_male_age_2020.csv\n",
      "Data for 2021 for sub_male_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_male_age_2021.csv\n",
      "Data for 2022 for sub_male_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_male_age_2022.csv\n",
      "Data for 2017 for sub_female_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_female_age_2017.csv\n",
      "Data for 2018 for sub_female_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_female_age_2018.csv\n",
      "Data for 2019 for sub_female_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_female_age_2019.csv\n",
      "Data for 2020 for sub_female_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_female_age_2020.csv\n",
      "Data for 2021 for sub_female_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_female_age_2021.csv\n",
      "Data for 2022 for sub_female_age has been written to /home/yujie0706/DataForGood-chicago/dfg_chi/backend/data_downloaded/Population/MedianAge/sub/sub_female_age_2022.csv\n"
     ]
    }
   ],
   "source": [
    "for name, variable in variables.items():    \n",
    "\n",
    "    ind_type = name.split('_')[0]\n",
    "\n",
    "    for year in range(2017, 2023):\n",
    "        # Construct the URL for the current year\n",
    "        if variable[5:][0] == 'B':    \n",
    "            url = f\"{base_url.format(year=year)}?get={variable}&{location}\"\n",
    "        elif variable[5:][0] == 'S':\n",
    "            url = f\"{subject_url.format(year=year)}?get={variable}&{location}\"\n",
    "        else:\n",
    "            url = f\"{profile_url.format(year=year)}?get={variable}&{location}\"\n",
    "            \n",
    "        # Make the request\n",
    "        response = requests.get(url)\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "        # relative path specifically for Contract Rent indicator\n",
    "        relative_path = os.path.join('data_downloaded', 'Population', 'MedianAge')\n",
    "\n",
    "        # Combine the current directory with the relative path\n",
    "        full_base_path = os.path.join(current_dir, relative_path)\n",
    "\n",
    "        # Specify the path to save the CSV file, one for each year\n",
    "        file_path = os.path.join(full_base_path, f'{ind_type}/{name}_{year}.csv')\n",
    "        \n",
    "        # Open a CSV file for writing for each year\n",
    "        with open(file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data into the CSV file\n",
    "            writer.writerows(data)\n",
    "\n",
    "        print(f\"Data for {year} for {name} has been written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate table saved\n",
      "sub_aggregate table saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3794545168.py:49: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data_downloaded/Population/MedianAge/\"\n",
    "\n",
    "for ind_type in ['main', 'sub']:\n",
    "    \n",
    "    short_dir = os.path.join(folder_path, f'{ind_type}')\n",
    "\n",
    "    if ind_type == 'main':    \n",
    "\n",
    "        for filename in os.listdir(short_dir):\n",
    "\n",
    "            if filename.endswith('.csv'):\n",
    "\n",
    "                filepath = os.path.join(short_dir, filename)\n",
    "            \n",
    "                indicator_name, year = extract_info_from_filename(filename, ind_type)\n",
    "\n",
    "                if indicator_name and year:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    # Create a new DataFrame with the required columns\n",
    "                    new_df = pd.DataFrame({\n",
    "                        'indicator_id': [int(2)] * len(df),\n",
    "                        'census_tract_id': df['tract'].astype(int),\n",
    "                        'indicator_name': indicator_name,\n",
    "                        'year': int(year),\n",
    "                        'value': df.iloc[:, 1].fillna(0).astype(int)\n",
    "                    })\n",
    "                    \n",
    "                    all_dataframes.append(new_df)\n",
    "\n",
    "        # Concatenate all DataFrames\n",
    "        final_dataframe = pd.concat(all_dataframes)\n",
    "\n",
    "        # Save to a new CSV file\n",
    "        final_dataframe.to_csv(os.path.join(short_dir, 'Main_Agg.csv'), index = False)\n",
    "        print('aggregate table saved')\n",
    "\n",
    "    else:\n",
    "\n",
    "        all_sub_dataframes = []  # List to store each file's DataFrame\n",
    "\n",
    "        for filename in os.listdir(short_dir):\n",
    "            if filename.startswith('sub') and filename.endswith('.csv'):\n",
    "                filepath = os.path.join(short_dir, filename)\n",
    "                sub_indicator_name, year = extract_info_from_filename(filename, ind_type)\n",
    "        \n",
    "                if sub_indicator_name and year:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    df['tract'] = pd.to_numeric(df['tract'], errors='coerce').fillna(0).astype(int)\n",
    "                    df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
    "            \n",
    "            # Create a new DataFrame with the required columns\n",
    "                    new_sub_df = pd.DataFrame({\n",
    "                        'indicator_id': [int(2)] * len(df),\n",
    "                        'census_tract_id': df['tract'].astype(int),\n",
    "                        'sub_group_indicator_name': sub_indicator_name,\n",
    "                        'year': int(year),\n",
    "                        'value': df.iloc[:, 1].fillna(0).astype(int)\n",
    "            })\n",
    "                    \n",
    "                    # Append this new DataFrame to the list\n",
    "                    all_sub_dataframes.append(new_sub_df)\n",
    "\n",
    "        # Concatenate all DataFrames for 'sub' into one\n",
    "        final_sub_dataframe = pd.concat(all_sub_dataframes, ignore_index=True)\n",
    "\n",
    "        # Save to a new CSV file\n",
    "        final_sub_dataframe.to_csv(os.path.join(short_dir, 'Sub_Agg.csv'), index=False)\n",
    "\n",
    "        print('sub_aggregate table saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Sub CSV file saved: Enrollment_Sub.csv\n",
      "Aggregated Main CSV file saved: Enrollment_Main.csv\n",
      "Aggregated Main CSV file saved: MedianEarning_Main.csv\n",
      "Aggregated Sub CSV file saved: MedianEarning_Sub.csv\n",
      "Aggregated Sub CSV file saved: HouseholdType_Sub.csv\n",
      "Aggregated Main CSV file saved: HouseholdType_Main.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Sub CSV file saved: ContractRent_Sub.csv\n",
      "Aggregated Main CSV file saved: ContractRent_Main.csv\n",
      "Aggregated Sub CSV file saved: MeanIncome_Sub.csv\n",
      "Aggregated Main CSV file saved: MeanIncome_Main.csv\n",
      "Aggregated Main CSV file saved: MedianIncome_Main.csv\n",
      "Aggregated Sub CSV file saved: MedianIncome_Sub.csv\n",
      "Aggregated Main CSV file saved: MedianAge_Main.csv\n",
      "Aggregated Sub CSV file saved: MedianAge_Sub.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
      "/tmp/user/23071/ipykernel_3258110/3546755849.py:34: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Sub CSV file saved: Races_Sub.csv\n",
      "Aggregated Main CSV file saved: Races_Main.csv\n",
      "Aggregated Main CSV file saved: Insurance_Main.csv\n",
      "Aggregated Sub CSV file saved: Insurance_Sub.csv\n",
      "Aggregated Main CSV file saved: Disability_Main.csv\n",
      "Aggregated Sub CSV file saved: Disability_Sub.csv\n"
     ]
    }
   ],
   "source": [
    "# save all agg files to a separate folder\n",
    "data_downloaded_path = \"data_downloaded\"\n",
    "indicator_id_map = {\"Economics\": 1, \"Education\": 2, \"Health\": 3, \"Housing\": 4, \"Population\": 5}\n",
    "output_dir = os.path.join(data_downloaded_path, 'Agg_data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Recursively search for CSV files in all subfolders\n",
    "for root, dirs, files in os.walk(data_downloaded_path):\n",
    "    agg_main_dataframes = []  # List to store dataframes for Agg_Main.csv\n",
    "    agg_sub_dataframes = []   # List to store dataframes for Agg_Sub.csv\n",
    "\n",
    "    # Check if the current directory is a largest directory under 'data_downloaded'\n",
    "    if os.path.dirname(root) == data_downloaded_path:\n",
    "        # Get the corresponding indicator_id for each directory\n",
    "        indicator_id = indicator_id_map.get(os.path.basename(root))\n",
    "        if indicator_id is None:\n",
    "            continue\n",
    "\n",
    "    if root.count('/') == 2:\n",
    "        sub_ind = root.strip('/').split('/')[-1]\n",
    "\n",
    "    for filename in files:\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            ind_type = os.path.basename(os.path.dirname(filepath))\n",
    "\n",
    "            indicator_name, year = extract_info_from_filename(filename, ind_type)\n",
    "\n",
    "            if indicator_name and year:\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Do we want to fill NAs with 0?\n",
    "                df['tract'] = pd.to_numeric(df['tract'], errors='coerce').fillna(0).astype(int)\n",
    "                df.iloc[:, 1] = pd.to_numeric(df.iloc[:, 1], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "                # Create dataframes for agg CSVs\n",
    "                if ind_type == 'main':\n",
    "                    new_main_df = pd.DataFrame({\n",
    "                        'indicator_id': [indicator_id] * len(df),\n",
    "                        'census_tract_id': df['tract'].astype(int),\n",
    "                        'indicator_name': indicator_name,\n",
    "                        'year': int(year),\n",
    "                        'value': df.iloc[:, 1].fillna(0).astype(int)\n",
    "                    })\n",
    "                    agg_main_dataframes.append(new_main_df)\n",
    "                elif ind_type == 'sub':\n",
    "                    new_sub_df = pd.DataFrame({\n",
    "                        'indicator_id': [indicator_id] * len(df),\n",
    "                        'census_tract_id': df['tract'].astype(int),\n",
    "                        'sub_group_indicator_name': indicator_name,\n",
    "                        'year': int(year),\n",
    "                        'value': df.iloc[:, 1].fillna(0).astype(int)\n",
    "                    })\n",
    "                    agg_sub_dataframes.append(new_sub_df)\n",
    "\n",
    "    # Concatenate all DataFrames for Agg_Main.csv and Agg_Sub.csv\n",
    "    if agg_main_dataframes:\n",
    "        final_main_dataframe = pd.concat(agg_main_dataframes, ignore_index=True)\n",
    "        output_filename = '_'.join([sub_ind, 'Main.csv'])\n",
    "        final_main_dataframe.to_csv(os.path.join(output_dir, output_filename), index=False)\n",
    "        print(f'Aggregated Main CSV file saved: {output_filename}')\n",
    "\n",
    "    if agg_sub_dataframes:\n",
    "        final_sub_dataframe = pd.concat(agg_sub_dataframes, ignore_index=True)\n",
    "        output_filename = '_'.join([sub_ind, 'Sub.csv'])\n",
    "        final_sub_dataframe.to_csv(os.path.join(output_dir, output_filename), index=False)\n",
    "        print(f'Aggregated Sub CSV file saved: {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Enrollment_Main.csv': ['enroll']\n",
      "'ContractRent_Main.csv': ['agg_rent']\n",
      "'Enrollment_Sub.csv': ['kind_12' 'grad' 'nursery' 'college']\n",
      "'MedianEarning_Sub.csv': ['less_high' 'high' 'bachelor' 'college' 'grad']\n",
      "'Disability_Sub.csv': ['cognitive' 'hearing' 'ambulatory' 'vision' 'ind_living' 'self_care']\n",
      "'Insurance_Sub.csv': ['uninsured' 'insured']\n",
      "'MedianAge_Main.csv': ['median_age']\n",
      "'MedianEarning_Main.csv': ['median_earning']\n",
      "'MedianAge_Sub.csv': ['male_age' 'female_age']\n",
      "'Disability_Main.csv': ['disability']\n",
      "'Races_Sub.csv': ['pop_asia' 'pop_black' 'pop_white' 'pop_two' 'pop_hawai' 'pop_ind_ala'\n",
      " 'pop_other']\n",
      "'MedianIncome_Sub.csv': ['median_hawai' 'median_other' 'median_asia' 'median_ind_ala'\n",
      " 'median_white' 'median_black']\n",
      "'MeanIncome_Main.csv': ['mean_income']\n",
      "'HouseholdType_Sub.csv': ['household_family' 'household_nonfamily']\n",
      "'Races_Main.csv': ['population']\n",
      "'HouseholdType_Main.csv': ['household_total']\n",
      "'MedianIncome_Main.csv': ['median_income']\n",
      "'MeanIncome_Sub.csv': ['mean_hawai' 'mean_other' 'mean_ind_ala' 'mean_white' 'mean_asia'\n",
      " 'mean_black']\n",
      "'ContractRent_Sub.csv': ['median_rent' 'lower_rent' 'upper_rent']\n",
      "'Insurance_Main.csv': ['population']\n"
     ]
    }
   ],
   "source": [
    "# This part is for checking the subgroups of each indicator # \n",
    "# Test Only #\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Replace 'path/to/your/folder' with the actual path to your folder\n",
    "folder_path = os.path.join(os.getcwd(), 'data_downloaded', 'Agg_data')\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Loop through the list of CSV files\n",
    "for csv_file in csv_files:\n",
    "        \n",
    "    # Read the CSV file into a DataFrame\n",
    "        file_name = os.path.basename(csv_file)\n",
    "\n",
    "\n",
    "        if file_name not in ['CensusTracts.csv', 'TractZipCodes.csv']:\n",
    "\n",
    "            df = pd.read_csv(csv_file)\n",
    "            main_part = file_name.split('_')[1].replace('.csv', '')\n",
    "\n",
    "            if main_part == 'Main':\n",
    "                column_name = 'indicator_name'  \n",
    "                unique_categories = df[column_name].unique()\n",
    "            else:\n",
    "                column_name = 'sub_group_indicator_name'  \n",
    "                unique_categories = df[column_name].unique()\n",
    "            \n",
    "            print(f\"'{file_name}': {unique_categories}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure different data sources are consistent on tracts and zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_shapefile_path = os.path.join(os.getcwd(), os.path.pardir, 'censustracts/censustracts.shp')\n",
    "tract_gdf = gpd.read_file(tract_shapefile_path)\n",
    "tract_gdf['tractce10'] = tract_gdf['tractce10'].astype(int)\n",
    "\n",
    "tract_file_path = os.path.join(os.getcwd(), 'data_downloaded/Agg_data/CensusTracts.csv')\n",
    "census_tract_df = pd.read_csv(tract_file_path)\n",
    "\n",
    "assert census_tract_df['tract_id'].unique() in tract_gdf['tractce10'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_shapefile_path = os.path.join(os.getcwd(), os.path.pardir, 'zipcode/zipcode.shp')\n",
    "zip_gdf = gpd.read_file(zip_shapefile_path)\n",
    "zip_gdf['zip'] = zip_gdf['zip'].astype(int)\n",
    "\n",
    "zip_file_path = os.path.join(os.getcwd(), 'data_downloaded/Agg_data/TractZipCodes.csv')\n",
    "zipcode_df = pd.read_csv(zip_file_path)\n",
    "\n",
    "assert zipcode_df['zip_code'].unique() in zip_gdf['zip'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing values and remove tract id not in CensusTracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data_downloaded/Agg_data'\n",
    "# census_tract_df = pd.read_csv(\"data_downloaded/Agg_data/CensusTracts.csv\")\n",
    "valid_tract_ids = set(census_tract_df['tract_id'])\n",
    "\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'value' in df:\n",
    "            # Replace negative values with NaN (which will be saved as NULL in the CSV)\n",
    "            df['value'] = df['value'].apply(lambda x: pd.NA if (x < 0 or pd.isna(x)) else int(x))\n",
    "            # filter out tract 80400\n",
    "            if 'census_tract_id' in df or 'zip_code' in df:\n",
    "                df = df[df['census_tract_id'].isin(valid_tract_ids)]\n",
    "    \n",
    "            # Save the updated DataFrame back to the CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"All CSV files in the 'Agg_data' folder have been updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check for negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negative_values = set()  # Use a set to store unique negative values\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the 'value' column, skip rows with missing 'value' data\n",
    "            df = pd.read_csv(file_path, usecols=['value'])\n",
    "            \n",
    "            # Filter out negative values and drop NA/None\n",
    "            negative_values = df['value'].dropna()[df['value'] < 0]\n",
    "            \n",
    "            # Update the set of all unique negative values\n",
    "            all_negative_values.update(negative_values.unique())\n",
    "        \n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"File '{filename}' is empty or missing critical data.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{filename}' not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file '{filename}': {str(e)}\")\n",
    "\n",
    "# Print all unique negative values found in all files\n",
    "print(\"All unique negative values across all CSV files:\")\n",
    "for value in sorted(all_negative_values):\n",
    "    print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
